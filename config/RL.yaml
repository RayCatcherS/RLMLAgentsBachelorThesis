behaviors:
  BehaviorTest:
    trainer_type: ppo
    
    hyperparameters:
      # Hyperparameters common to PPO and SAC
      batch_size: 1024
      buffer_size: 20480
      learning_rate: 0.0003
      learning_rate_schedule: linear
      # PPO-specific hyperparameters
      beta: 0.02
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3

    # Configuration of the neural network (common to PPO/SAC)
    network_settings:
      normalize: false
      hidden_units: 256
      num_layers: 3
      vis_encode_type: simple

    # Trainer configurations common to all trainers
    max_steps: 1300000  # Step massimi per l'addestramento
    keep_checkpoints: 5
    time_horizon: 64
    summary_freq: 60000

    # self-play
    self_play:
      save_steps: 100000
      play_against_latest_model_ratio: 0.35
      window: 4

    reward_signals:
      # environment reward (default)
      extrinsic:
        gamma: 0.99
        strength: 1.0



# indice valori apprendimento[index environment, qta colpi]
environment_parameters:
  my_environment_parameter:
    curriculum:
      # Prima lezione (iniziale)
      - name: lesson0
        completion_criteria:
          measure: progress
          behavior: BehaviorTest
          threshold: 0.30  # Cambia lezione al 10% del training 
        value: 0
      # Seconda lezione
      - name: lesson1
        completion_criteria:
          measure: progress
          behavior: BehaviorTest
          threshold: 0.40  # Cambia lezione al 20% del training 
        value: 1
      # Terza lezione
      - name: lesson2
        completion_criteria:
          measure: progress
          behavior: BehaviorTest
          threshold: 0.60  # Cambia lezione al 50% del training 
        value: 2
      # Quarta lezione
      - name: lesson3
        completion_criteria:
          measure: progress
          behavior: BehaviorTest
          threshold: 0.99  # Cambia lezione al 70% del training 
        value: 3
  
